<configuration>
	<property>
		<name>dfs.namenode.secondary.http-address</name>
		<value>master:9001</value>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:/usr/local/hadoop-2.8.3/dfs/name</value>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>file:/usr/local/hadoop-2.8.3/dfs/data</value>
	</property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/usr/local/hadoop-2.8.3/tmp</value>
    </property>
	<property>
		<name>dfs.repliction</name>
		<value>3</value>
	</property>
	<!--关闭权限检查-->
	<property>
		<name>dfs.permisstions.enable</name>
		<value>false</value>
	</property>
    <property>
		<name>dfs.permisstions</name>
		<value>false</value>
	</property>
	<!--完全分布式集群名称-->
	<property>  
        <name>dfs.nameservices</name>  
        <value>master</value>  
    </property> 
	<property>  
        <name>dfs.ha.namenodes.master</name>  
        <value>master1,master2</value>  
    </property> 
	<!-- master1的RPC通信地址，master1所在地址  -->  
    <property>  
        <name>dfs.namenode.rpc-address.master.master1</name>  
        <value>master1:9000</value>  
    </property>  
    <!-- master1的http通信地址，外部访问地址 -->  
    <property>  
        <name>dfs.namenode.http-address.master.master1</name>  
        <value>master1:50070</value>  
    </property>
	<!-- master2的RPC通信地址，master2所在地址  -->  
    <property>  
        <name>dfs.namenode.rpc-address.master.master2</name>  
        <value>master2:9000</value>  
    </property>  
    <!-- master2的http通信地址，外部访问地址 -->  
    <property>  
        <name>dfs.namenode.http-address.master.master2</name>  
        <value>master2:50070</value>  
    </property>
	<!-- 指定NameNode的元数据在JournalNode日志上的存放位置(一般和zookeeper部署在一起) -->  
    <property>  
        <name>dfs.namenode.shared.edits.dir</name>  
        <value>qjournal://slave1:8485;slave2:8485;slave3:8485/master</value>  
    </property>  
    <!-- 指定JournalNode在本地磁盘存放数据的位置 -->  
    <property>  
        <name>dfs.journalnode.edits.dir</name>  
        <value>/usr/local/hadopp-2.8.3/dfs/data/journal</value>  
    </property> 
	<!-- 客户端连接可用状态的NameNode所用的代理类 -->
	<property>
		<name>dfs.client.failover.proxy.provider.master</name>
		<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	</property>
    <!--配置隔离机制方法，同一时刻只能有一台服务器对外响应，多个机制用换行分隔，即每个机制暂用一行,也可用shell命令切换-->
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
	<!-- 这个是使用sshfence隔离机制时才需要配置ssh免登陆 -->  
    <property>  
        <name>dfs.ha.fencing.ssh.private-key-files</name>  
        <value>/root/.ssh/id_rsa</value>  
    </property>
     <!-- 配置sshfence隔离机制超时时间 -->
    <property>
        <name>dfs.ha.fencing.ssh.connect-timeout</name>
        <value>30000</value>
    </property>  
	<!-- 这个是开启自动故障转移，如果你没有自动故障转移，这个可以先不配 -->  
    <property>  
        <name>dfs.ha.automatic-failover.enabled</name>  
        <value>true</value>  
    </property>
    <property>  
        <name>dfs.qjournal.start-segment.timeout.ms</name>  
        <value>90000</value>  
    </property>
    <property>  
        <name>dfs.qjournal.select-input-streams.timeout.ms</name>  
        <value>90000</value>  
    </property>
    <property>  
        <name>dfs.qjournal.write-txns.timeout.ms</name>  
        <value>90000</value>  
    </property>
    

</configuration>
